# learnGPT - week1

## Requirements

### Python version
```bash
python --version
```
```text
Python 3.9.12
```
### Libraries used
```bash
pip3 install torch==1.13.1 pytest==7.2.1 pyyaml==6.0
```

## TODO's
### TODO 1 - a naive LM (`GPTVer1`)

```bash
pytest tests/test_1.py -s -vv
```


### TODO 2 - taking the past into account  (`HeadVer1` & `GPTVer2`)

```bash
pytest tests/test_2.py -s -vv
```


### TODO 3 - vectorizing for loops (`HeadVer2`)

```bash
pytest tests/test_3.py -s -vv
```

### TODO 4 - taking the past into account with masking & normalization (`HeadVer3`)

```bash
pytest tests/test_4.py -s -vv
```

### TODO 5 - self-attention mechanism (`HeadVer4` & `GPTVer2`)

```bash
pytest tests/test_5.py -s -vv
```

### TODO 6 - positional encodings (`GPTVer3`)

```bash
pytest tests/test_6.py -s -vv
```

<br>

## Introduction
<img src='img/week1.png' width=300>

Onboarding Course week1ì—ì„œëŠ” character leve tokenizerë¥¼ ì‚¬ìš©í•˜ëŠ” Simplest BigramLanguageModelì„ ë°œì „ì‹œì¼œë³´ê³  í•©ë‹ˆë‹¤.

DataëŠ” Shakespeareì˜ í¬ê³¡ ëŒ€ë³¸ì„ input.txtë¡œ ì œê³µí•´ë“œë ¸ì§€ë§Œ, ì›í•˜ëŠ” textë¥¼ ì‚¬ìš©í•˜ì…”ë„ ë¬´ê´€í•©ë‹ˆë‹¤.

week1ì˜ ìµœì¢… ëª©í‘œëŠ” one-head self-attention(HeadVer4)ë¥¼ ì ìš©í•œ GPTVer3ë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

## Tokenization
ìì—°ì–´ì²˜ë¦¬ë¥¼ ìœ„í•´ì„  ë§ë­‰ì¹˜ë¥¼ ì ì ˆíˆ ì „ì²˜ë¦¬ í•´ì•¼í•©ë‹ˆë‹¤. 

Tokenizationì€ ì „ì²˜ë¦¬ ê³¼ì •ì˜ ì‹œì‘ìœ¼ë¡œ, ë§ë­‰ì¹˜ë¥¼ tokenì´ë¼ëŠ” ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì£¼ëŠ” ì‘ì—…ì„ ë§í•©ë‹ˆë‹¤. ê·¸ëŸ°ë° í† í°í™”ë¥¼í•˜ë‹¤ë³´ë©´ ì—¬ëŸ¬ ì„ íƒì˜ ìˆœê°„ì´ ë°œìƒí•©ë‹ˆë‹¤. ê°€ë ¹ *I donâ€™t like olive* ë¼ëŠ” ë¬¸ì¥ì´ ì¡´ì¬í•  ë•Œ *donâ€™t*ë¥¼ í† í°í™”í•˜ëŠ” ë°©ë²•ì€ ë‹¤ì–‘í•©ë‹ˆë‹¤. 

- don / t
- dont
- do  / nâ€™t

trainí•  ì „ì²´ textë¥¼ tokenizationí•˜ë©´ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ vocabularyë¥¼ ë§Œë“¤ì–´ tokenì„ ì •ìˆ˜ë¡œ ì¸ì½”ë”©ì„ í•©ë‹ˆë‹¤. ê·¸ëŸ°ë° inferenceë¥¼ í•˜ëŠ” ê³¼ì •ì—ì„œ ê°€ë ¹ ì˜¤íƒ€ì™€ ê°™ì´ vocabularyì— ì—†ëŠ” ë‹¨ì–´(train ë‹¨ê³„ì—ì„œ ì ‘í•˜ì§€ ëª»í•œ ë‹¨ì–´)ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° OOV(Out-Of-Vocabulary) ë¬¸ì œê°€ ë°œìƒí•˜ê²Œ ë©ë‹ˆë‹¤.

ìœ„ì™€ ê°™ì€ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ tokenization ì „ëµì´ ì—°êµ¬ë˜ì—ˆê³ , ê·¸ì¤‘ BPE(Byte Pair Encoding)ë¥¼ main_bpe.pyì— ê°„ë‹¨íˆ êµ¬í˜„í•´ë´¤ìŠµë‹ˆë‹¤. BPEë¡œ vocabularyë¥¼ êµ¬ì„±í•˜ë©´ ìœ„ì™€ ê°™ì´ ì´ˆë°˜ì—ëŠ” vocabulary sizeê°€ ì¦ê°€í•˜ë‹¤ê°€ ì ì  ê°ì†Œí•˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤.

<img src='img/BPE.png' width=300>

```text
ğŸ’¡ ë³¸ ì½”ìŠ¤ì—ì„  Character level tokenizerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
```

## GPTVer1
`GPTVer1`ì€ `context`ê°€ ì£¼ì–´ì§€ê³  ë‹¤ìŒ tokenì„ ì˜ˆì¸¡ í•  ë•Œ ì˜¤ì§ ì§ì „ tokenë§Œì„ ì°¸ê³ í•˜ëŠ” BigramLMì…ë‹ˆë‹¤. 

$$
P(w|like) =  {count(like , w)\over count(like)}
$$

ê°€ë ¹ `context` ì—ì„œ likeê°€ 10ë²ˆ ë“±ì¥í–ˆê³ , like appleì´ 8ë²ˆ like orangeê°€ 2ë²ˆ ë“±ì¥í–ˆë‹¤ë©´ $P(apple|like)$ ëŠ” 0.8,  $P(orange|like)$ ëŠ” 0.2ê°€ ë©ë‹ˆë‹¤. BigramLMì€ ì´ì²˜ëŸ¼ ë§ˆì§€ë§‰ tokenì— ëŒ€í•´ ë‹¤ìŒì— ì˜¬ tokenì— ëŒ€í•œ í™•ë¥ ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ëŠ” LMì…ë‹ˆë‹¤.

`GPTVer1` ì—ì„  (`vocab_size` x `vocab_size`)ì˜ `token_embedding_table`ì„ ë§Œë“¤ê³  ì´ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ê° tokenì˜ ì •ìˆ˜ ì¸ì½”ë”© ê°’ì€ tableì˜ indexì— ëŒ€ì‘í•˜ê³ , `token_embedding_table(index)`ëŠ” ë‹¤ìŒì— ë‚˜ì˜¬ í† í°ì— ëŒ€í•œ ë¡œì§“ê°’ì´ ë©ë‹ˆë‹¤.

Language Modelë¡œ textë¥¼ `generate`í•  ë•Œ classificationì²˜ëŸ¼ max probabilityë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í™•ë¥ ë¶„í¬ì—ì„œ ë‹¤ìŒ í† í°ì„ ì„ì˜ì¶”ì¶œí•©ë‹ˆë‹¤. 

GPTVer1ì—ì„œëŠ” generate í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

```bash
# hint
idx_next = torch.multinomial(probs, num_samples=1)
```

## HeadVer1 & GPTVer2

`GPTVer1` ì€ ì§ì „ tokenì˜ ì •ë³´ë§Œì„ í† ëŒ€ë¡œ ë‹¤ìŒ tokenì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ì´ ê²½ìš° ì• ë¶€ë¶„ê³¼ ë’· ë¶€ë¶„ì˜ ë¬¸ë§¥ì´ ì „í˜€ ì—°ê²°ë˜ì§€ ì•ŠëŠ” ê²½ìš°ë„ ìƒê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

`GPTVer2`ì™€ `HeadVer1`ì—ì„œëŠ” ì§ì „ tokenê³¼ í•¨ê»˜ ì´ì „ tokenë“¤ì˜ embedding vectorì˜ í‰ê· ìœ¼ë¡œ ë‹¤ìŒ tokenì„ ì˜ˆì¸¡í•˜ê³ ì í•©ë‹ˆë‹¤. ê°€ë ¹ 4th tokenì„ ì˜ˆì¸¡í•˜ê³ ì í•˜ë©´ 1st, 2nd, 3rd tokenì˜ embedding vectorì˜ í‰ê· ìœ¼ë¡œ ë‹¤ìŒ tokenì„ ì˜ˆì¸¡í•˜ê³ ì í•©ë‹ˆë‹¤.

HeadVer1ì—ì„œëŠ” ì´ë¥¼ **2ì¤‘ forë¬¸ì„ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•´ì£¼ì„¸ìš”**

`hint: Each example across batch dimension is of course processed completely independently and never "talk" to each other`

GPTVer2ì—ì„œëŠ” embedding tableì´ (`vocab_size` x `embed_size`)ë¡œ ë³€ê²½ëìŠµë‹ˆë‹¤. ìµœì¢… ë‹¨ê³„ì—ì„œ fully connected layer(self.lm_head)ë¥¼ ì‚¬ìš©í•˜ì—¬ logitsì„ (`batch_size`, `block_size`, `vocab_size`)ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

`hint: embedding table â†’ Head â†’ FC`

## HeadVer2
HeadVer2ì—ì„œëŠ” HeadVer1ì„ **matrix multiplicationì„ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•´ì£¼ì„¸ìš”**

`hint: torch.tril(...)`

## HeadVer3
HeadVer3ì—ì„œëŠ” HeadVer2ë¥¼ **softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•´ì£¼ì„¸ìš”**

`hint: softmax([0, 0, -inf, -inf]) = [0.5, 0.5, 0, 0]`

## HeadVer4 (Self-attention Head)
HeadVer4ì—ì„œëŠ” **self-attention headë¥¼ êµ¬í˜„í•´ë³´ê³ ì í•©ë‹ˆë‹¤. softmaxë¥¼ ì ìš©í•  ë•Œ HeadVer3ì˜ ì „ëµì„ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•´ì£¼ì„¸ìš”.**

$$
Attention(Q, K, V) = softmax({QK^T \over \sqrt d_k})V
$$

TODO ìœ„ì˜ if debug: ëŠ” test codeë¥¼ ìœ„í•œ ë¶€ë¶„ì´ë‹ˆ, ì´ ë¶€ë¶„ì€ ìˆ˜ì •í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.

HeadVer4ì—ì„œëŠ” scailingì„ ì˜ í–ˆëŠ”ì§€ì™€ softmaxë¥¼ ì˜ ì ìš©í–ˆëŠ”ì§€ì— ëŒ€í•´ testë¥¼ í•©ë‹ˆë‹¤. ì´ë•Œ test code ì ìš©ì— í•„ìš”í•œ parameterë¥¼ ì €ì¥í•´ì£¼ì…”ì•¼í•©ë‹ˆë‹¤. ì•„ë˜ëŠ” pseudo codeì´ê³  ê° ì—°ì‚°ì„ ë§ˆë¬´ë¦¬í•˜ê³  í•´ë‹¹ ë³€ìˆ˜ì— ì €ì¥í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

1. self.var = (Q@K^T / sqrt(d_k)).var() â† scailing ì´í›„ varianceë¥¼ ì €ì¥í•˜ì‹œë©´ ë©ë‹ˆë‹¤.
2. self.wei = Attention(Q, K, V) â† ìµœì¢… weië¥¼ ì €ì¥í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

`hint: "Scaled" attention additional dividesÂ weiÂ by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much.`

## GPTVer3
ì™„ì„±í•œ HeadVer4ë¥¼ ì‚¬ìš©í•˜ì—¬ GPTVer3 êµ¬í˜„í•˜ê³ ì í•©ë‹ˆë‹¤.

ìœ„ì—ì„œ êµ¬í˜„í•œ HeadVer4ëŠ” ê³¼ê±°ì˜ ì •ë³´ë¥¼ ë°˜ì˜ì„ í•˜ì§€ë§Œ, RNNê³¼ ë‹¬ë¦¬ Sequential dataë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Positional encodingê³¼ ìµœì¢… logitsì„ êµ¬í˜„í•´ë´…ë‹ˆë‹¤.**

GPTVer3ì—ì„œë„ embedding tableì˜ shapeì´ (`vocab_size` x `embed_size`) ì…ë‹ˆë‹¤. ìµœì¢… ë‹¨ê³„ì—ì„œ fully connected layer(self.lm_head)ë¥¼ ì‚¬ìš©í•˜ì—¬ logitsì„ (`batch_size`, `block_size`, `vocab_size`)ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

### Positional encoding

[https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)

[https://www.notion.so/WEEK9-How-does-Positional-Encoding-work-0d0e5b9d17464af08f39b4977c073beb#f77f059d74ae45599eca16c1b3924e91](https://www.notion.so/0d0e5b9d17464af08f39b4977c073beb)

### Logits
```bash
# pseudo code
x = tok_emb + pos_emb
x = head(x)
logits = FC(x)
```
